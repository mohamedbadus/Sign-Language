# Sign Language Recognition App using CNN

This Flutter project implements a mobile application for sign language recognition using machine learning models. Users can use the app to capture videos of sign language gestures, and the app will analyze them to recognize the corresponding signs. Below is a README.md template to guide you in creating the documentation for your GitHub repository.

# Overview

This Flutter application utilizes machine learning models for sign language recognition. Users can record videos of sign language gestures using their device's camera, and the app will analyze the videos to recognize the signs being performed. The app provides real-time feedback on the recognized signs, helping users learn sign language or communicate effectively with the hearing impaired.
![image](https://github.com/mohamedbadus/Plant-disease-detection/assets/116250693/e40e6e99-43df-444a-962d-d4acc26ed836)


# Features
1. Record videos of sign language gestures using the device's camera.
2. Analyze sign language videos to recognize the performed signs using machine learning models.
3. Display real-time results indicating the recognized signs.
4. Provide translations or descriptions of recognized signs to assist users in learning sign language.

# Installation

1. Clone this repository:
`git clone https://github.com/mohamedbadus/Sign-Language.git`
2. Navigate to the project directory:
`cd Sign-Language`
3. Install dependencies:
`flutter pub get`

# Usage
1. Run the application on an emulator or a connected device:
`flutter run`
2. Use the app to record videos of sign language gestures.
3. View the real-time results provided by the app, indicating the recognized signs.

# Output Screenshot
![WhatsApp Image 2024-04-03 at 20 49 55_747c087e](https://github.com/mohamedbadus/Sign-Language/assets/116250693/5a50b7ec-911c-45da-87b0-0f128ff458f9)



